{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7fb982e",
   "metadata": {},
   "source": [
    "# Create downstream tasks\n",
    "This Jupyter Notebook hepls to create new downstream tasks for models that are trained on Wikidata5M.\n",
    "\n",
    "Required files for this Jupyter Notebook to work.\n",
    "1. Two mapping CSV files which contains information about all entities in Wikidata5M and all relatins for the entities in Wikidata5M. You can check the 02_csv_mapping_entities.csv and 02_csv_mapping_relations.csv file for the format or just use it directly.\n",
    "3. A folder containing a JSON file for each entity in Wikidata5M and all relations.\n",
    "\n",
    "## How does this Notebook work?\n",
    "The notebook is divided into five different sections. Sections 1, 2, 3 and 5 start with a cell where you need to set variables according to the dataset you want to create. Section 4 is a room for you to pre-process the data if you need to. If you don't need to change anything you can skip section 4.\n",
    "\n",
    "To create a dataset you have to go through the Notebook and run each cell. You need to set variables in the cells that start with #please set variables. And es mentioned before, section 4 is for you to pre-process your data the way you need it.\n",
    "E.g.: Transforming a date into a number to create a regression downstream task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9178448",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import sys\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4499815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#please set variables\n",
    "\n",
    "#Filename of the mapping CSV which contains the information about the entities.\n",
    "mapping_file_entities = '02_csv_mapping_entities.csv'\n",
    "mapping_file_relations = '02_csv_mapping_relations.csv'\n",
    "\n",
    "#Folder which contains all the JSON files for every entity\n",
    "json_download_folder = '01_download_entities_relations/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a34a302",
   "metadata": {},
   "outputs": [],
   "source": [
    "#no need to change anything here\n",
    "#reading all entities and relations from csv\n",
    "df = pd.read_csv(mapping_file_entities, delimiter=';')\n",
    "\n",
    "#cleaning up dataframes\n",
    "df['instance_of'] = df['instance_of'].replace(\"'\", \"\", regex=True).replace(\"\\[\", \"\", regex=True).replace(\"\\]\", \"\", regex=True).replace(\", \", \",\", regex=True)\n",
    "df['relations_wikidata'] = df['relations_wikidata'].replace(\"'\", \"\", regex=True).replace(\"\\[\", \"\", regex=True).replace(\"\\]\", \"\", regex=True).replace(\", \", \",\", regex=True)\n",
    "df['relations_wikidata5m'] = df['relations_wikidata5m'].replace(\"'\", \"\", regex=True).replace(\"\\[\", \"\", regex=True).replace(\"\\]\", \"\", regex=True).replace(\", \", \",\", regex=True)\n",
    "df['entity_id'] = df['entity_id'].replace(\"'\", \"\", regex=True)\n",
    "df['instance_of'] = df['instance_of'].apply(lambda x: x.split(',')).to_frame()\n",
    "df['relations_wikidata'] = df['relations_wikidata'].apply(lambda x: x.split(',')).to_frame()\n",
    "df['relations_wikidata5m'] = df['relations_wikidata5m'].apply(lambda x: x.split(',')).to_frame()\n",
    "df = df.set_index('entity_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9aa2df",
   "metadata": {},
   "source": [
    "# 1.) Filtering for downstreamtask.\n",
    "The DataFrame is filtered for the relation you put in and potentially for the entity type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "acfcf441",
   "metadata": {},
   "outputs": [],
   "source": [
    "#please set variables\n",
    "\n",
    "#Relation for the downstream task. Label and Wikidata ID is needed!\n",
    "#E.g.: \n",
    "#downstream_task_relation_label = 'P21'\n",
    "#downstream_task_relation_label = 'sex or gender'\n",
    "relation_id = 'P2044'\n",
    "relation_label = 'elevation above sea level\t'\n",
    "\n",
    "\n",
    "#Entity type for which you want to create the downstream task. \n",
    "#E.g.: \n",
    "#entity_type_id = 'Q5'\n",
    "#entity_type_label = 'human'\n",
    "entity_type_id = 'Q1248784'\n",
    "entity_type_label = 'airport'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "fc133ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11649 entities fit criteria\n"
     ]
    }
   ],
   "source": [
    "#filtering dataframe for \n",
    "df_sample = df.copy()\n",
    "df_sample = df_sample[df_sample.instance_of.apply(lambda x: entity_type_id in x)]\n",
    "df_sample = df_sample[df_sample.relations_wikidata5m.apply(lambda x: relation_id not in x)]\n",
    "df_sample = df_sample[df_sample.relations_wikidata.apply(lambda x: relation_id in x)]\n",
    "\n",
    "#amount of entities that fit cirteria for downstream task\n",
    "print(len(df_sample), 'entities fit criteria')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f71cab",
   "metadata": {},
   "source": [
    "# 2.) Creating a sample for the data set.\n",
    "In the cell above this one you can read how many entities fit the filtered critarea. Based on this you can now decide how big your sample_size should be.\n",
    "In this section the sample based on your random seed and sample size will be created.\n",
    "The last cell prints out the content of the JSON area containing information about the relation. Since there are diffrent JSON structures for different relation you will need to set what item you need in the next section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "dc0f6119",
   "metadata": {},
   "outputs": [],
   "source": [
    "#please set variables\n",
    "\n",
    "#random seed\n",
    "seed = 4\n",
    "\n",
    "#sample size for dataset\n",
    "sample_size = 11649"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "dfafaf65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity ID: Q3433938\n",
      "Content of JSON: {'amount': '+13', 'unit': 'http://www.wikidata.org/entity/Q11573'}\n"
     ]
    }
   ],
   "source": [
    "#Creating the sample of the data.\n",
    "df_sample = df_sample['relations_wikidata'].sample(n = sample_size, random_state = seed).to_frame()\n",
    "df_sample['relations_wikidata'] = relation_id\n",
    "\n",
    "#Printing out \n",
    "try:\n",
    "    file = open(json_download_folder + df_sample.index[0] + '.json', encoding='UTF-8')\n",
    "    data = json.load(file)\n",
    "    key = next(iter(data['entities'].keys()))\n",
    "    print('Entity ID:', key)\n",
    "    print('Content of JSON:', data['entities'][key]['claims'][relation_id][0]['mainsnak']['datavalue']['value'])\n",
    "    file.close()\n",
    "except Exception as e:\n",
    "    print(index, e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4da1f3",
   "metadata": {},
   "source": [
    "# 3.) Set element which contains the value\n",
    "This section maps the entity IDs to the values of the relation. \n",
    "In this step you need to figure out what information of the JSON file you need. Here are two examples:\n",
    "Depending on the sample size this step may take a bit.\n",
    "#### Example: date_of_birth\n",
    "In this example we want to build a downstream task for the relation date_of_birth. The output of the above cell should look somewhat like this:\n",
    "\n",
    "Content of JSON: {'time': '+1930-01-11T00:00:00Z', 'timezone': 0, 'before': 0, 'after': 0, 'precision': 11, 'calendarmodel': 'http://www.wikidata.org/entity/Q1985727'}\n",
    "\n",
    "\n",
    "Since we are looking for the date we have to choose the element \"time\" which contains the data we want to use: +1930-01-11T00:00:00Z\n",
    "\n",
    "So we set the set the variable: value_in_json = \"time\"\n",
    "\n",
    "#### Example: sex_or_gender\n",
    "In this example we want to build a downstream task for the relation sex_or_gender. The output of the above cell should look somewhat like this:\n",
    "\n",
    "Content of JSON: {'entity-type': 'item', 'numeric-id': 6581072, 'id': 'Q6581072'}\n",
    "\n",
    "Since we are looking for the gender we have to choose the element \"id\" which contains the ID of the gender entity:\n",
    "Q6581072 (which is the ID for 'female')\n",
    "\n",
    "So we set the set the variable: value_in_json = \"time\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "e8f5f1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#please set variable\n",
    "value_in_json = 'amount'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c66dbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def getValue(row):\n",
    "    try:\n",
    "        file_name = json_download_folder + row.entity_id + \".json\"\n",
    "        file = open(file_name, encoding='UTF-8')\n",
    "        data = json.load(file)\n",
    "        key = next(iter(data[\"entities\"].keys()))\n",
    "        values = []\n",
    "        for relation in data[\"entities\"][key][\"claims\"][relation_id]:\n",
    "            value = relation[\"mainsnak\"][\"datavalue\"][\"value\"][value_in_json]\n",
    "            values.append(value)\n",
    "        file.close()\n",
    "        return values\n",
    "    except Exception as e:\n",
    "        pass\n",
    "        \n",
    "df_sample = df_sample.reset_index()\n",
    "df_sample['value'] = df_sample.apply(lambda row: getValue(row), axis=1)\n",
    "df_sample = df_sample.set_index('entity_id')\n",
    "df_sample = df_sample.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f028f7",
   "metadata": {},
   "source": [
    "# 4.) Room for manipulation of the data value\n",
    "You might need to manupilate the data to get a suitable format. \n",
    "In the cell above you see how your current sample set looks like. Here you have room to finalise your dataset.\n",
    "\n",
    "E.g.: You may still have some empty fields you want to clear out. You may want to simplify your dataset by filtering out for specific results. You may want to pre-process your data to fit the format you need for your downstream task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444d64dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3444f71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop entities with multiple values to clean up data. they are inconsistently saved.\n",
    "df_sample['count'] = df_sample.apply(lambda x: len(x.value), axis=1)\n",
    "df_sample = df_sample[df_sample['count'] == 1]\n",
    "df_sample = df_sample.drop('count', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2723e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3936966f",
   "metadata": {},
   "source": [
    "# 5.) Creating triples file\n",
    "In this section the triples file and the readme file are being creating:\n",
    "\n",
    "all_triples.csv: Contains three columns. In the first column contains the entity ID. The second column contains the relation ID. And the third column the relation value. The relation value is saved in a list since one entity can have more than one value per relation.\n",
    "\n",
    "README.md: The readme file contains information about which variables you set in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "d9364260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'07_datasets/album_producer/05_notebook_downstream_task_creator.ipynb'"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"07_datasets/\" + \"_\".join(entity_type_label.split()) + \"_\" + \"_\".join(relation_label.split())\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "df_sample.to_csv(path + \"/all_triples.csv\", sep=\";\", index=True, header=False)\n",
    "with open(os.path.join(path, \"README.md\"), \"w\") as file:\n",
    "    file.write(\"Dataset created with jupyter notebook 05_notebook_downstreak_task_creator\\n\")\n",
    "    file.write(\"Variables in notebook:\\n\")\n",
    "    file.write(\"relation_id:                           {}\\n\".format(relation_id))\n",
    "    file.write(\"relation_label:                        {}\\n\".format(relation_label))\n",
    "    file.write(\"entity_type_id:                        {}\\n\".format(entity_type_id))\n",
    "    file.write(\"entity_type_label:                     {}\\n\".format(entity_type_label))\n",
    "    file.write(\"seed:                                  {}\\n\".format(seed))\n",
    "    file.write(\"sample_size (variable):                {}\\n\".format(sample_size))\n",
    "    file.write(\"actual size (df_sample.dropna()):      {}\\n\".format(len(df_sample)))\n",
    "    file.write(\"json_download_folder:                  {}\\n\".format(json_download_folder))\n",
    "    file.write(\"value_in_json:                         {}\\n\".format(value_in_json))\n",
    "#copying this file to sub folder\n",
    "shutil.copyfile('05_notebook_downstream_task_creator.ipynb', path + '/05_notebook_downstream_task_creator.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c941faa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
